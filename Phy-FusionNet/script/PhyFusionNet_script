# If needed, install required packages as below
# pip install imblearn
# pip install -U imbalanced-learn
# pip install --upgrade scikit-learn
# =================================================================================================
# Phy-FusionNet Script
#
# This script implements the Phy-FusionNet architecture:
# 1.  Physiological Data Processing: Handles loading, cleaning, SMOTE for imbalance, and scaling.
# 2.  Fourier Analysis Module: Implements Fourier Positional Encoding and Frequency-Aware Attention.
# 3.  Memory Stream Module: Includes a Memory Encoder, a stateful Memory Bank with a FIFO queue
#     and decay updates, and a Memory-Conditioned Attention mechanism.
# 4.  Adaptive Temporal Attention: Uses a Mixture-of-Head (MOH) attention mechanism.
# 5.  End-to-End Training: Incorporates Focal Loss, AdamW, ReduceLROnPlateau scheduler, early
#     stopping, and mixed-precision training.
# 6.  Hyperparameter Optimization: Integrates Optuna for automated tuning.
# 7.  Comprehensive Logging: Logs all configurations, metrics, and artifacts.
# =================================================================================================

import math
import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from imblearn.over_sampling import SMOTE
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from collections import deque
import warnings
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
from torch.amp import GradScaler, autocast
from torch.utils.tensorboard import SummaryWriter
import matplotlib.pyplot as plt
import seaborn as sns
import optuna
import optuna.visualization as vis
from scipy import signal, stats
import sys
import json
import time
import datetime
import platform
import random

# ==============================================================================================
# 0. Global Setup: Seeding and Logging
# ==============================================================================================
# Set global random seeds for reproducibility
SEED = 42
np.random.seed(SEED)
random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# Suppress minor warnings
warnings.filterwarnings('ignore', category=UserWarning, message='.*nonzero.*')

# Define file paths and constants
# Using the provided preprocessed PPB-Emo data fragment
PHYSIO_FILE = 'input/dataset/merged_data.csv'
OUTPUT_DIR = 'experiment_output'
CHECKPOINT_DIR = os.path.join(OUTPUT_DIR, 'checkpoints')
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(CHECKPOINT_DIR, exist_ok=True)

# Tee all stdout/stderr to a log file for complete record-keeping
class _Tee:
    def __init__(self, *streams):
        self.streams = streams
    def write(self, data):
        for s in self.streams:
            try:
                s.write(data)
                s.flush()
            except Exception:
                pass
    def flush(self):
        for s in self.streams:
            try:
                s.flush()
            except Exception:
                pass

_log_path = os.path.join(OUTPUT_DIR, 'experiment_console.log')
_sys_stdout = sys.stdout
_sys_stderr = sys.stderr
try:
    _log_fp = open(_log_path, 'a', buffering=1, encoding='utf-8')
    sys.stdout = _Tee(sys.stdout, _log_fp)
    sys.stderr = _Tee(sys.stderr, _log_fp)
    print("\n" + "="*100)
    print(f"[{datetime.datetime.now().isoformat()}] Experiment logging started. Console tee -> {_log_path}")
    print("="*100 + "\n")
except Exception as _e:
    print(f"[LOG INIT WARNING] Failed to initialize tee logger: {_e}")

# Lightweight experiment logger for saving configuration, metrics, and artifacts
class ExperimentLogger:
    def __init__(self, out_dir):
        self.out_dir = out_dir
        self.payload = {
            "timestamp": datetime.datetime.now().isoformat(),
            "seed": SEED,
            "artifacts": {},
            "metrics": {},
            "config": {},
            "software": {},
            "hardware": {},
            "notes": []
        }
        self._t0 = time.time()

    def set(self, key, value):
        self.payload[key] = value

    def put(self, section, key, value):
        if section not in self.payload:
            self.payload[section] = {}
        self.payload[section][key] = value

    def add_note(self, txt):
        self.payload.setdefault("notes", []).append(str(txt))

    def add_artifact(self, key, path):
        self.payload.setdefault("artifacts", {})[key] = path

    def finalize(self):
        self.payload["runtime_sec"] = round(time.time() - self._t0, 3)
        out = os.path.join(self.out_dir, "experiment_summary.json")
        try:
            with open(out, "w", encoding="utf-8") as f:
                json.dump(self.payload, f, ensure_ascii=False, indent=2)
            print(f"[ExperimentLogger] Summary saved -> {out}")
        except Exception as e:
            print(f"[ExperimentLogger] Failed to save summary: {e}")

EXP_LOG = ExperimentLogger(OUTPUT_DIR)
EXP_LOG.put("software", "python", sys.version)
EXP_LOG.put("software", "pandas", pd.__version__)
EXP_LOG.put("software", "numpy", np.__version__)
EXP_LOG.put("software", "torch", torch.__version__)
EXP_LOG.put("software", "optuna", optuna.__version__)
EXP_LOG.put("hardware", "platform", platform.platform())

# ==============================================================================================
# 1. Data Processing Pipeline
# ==============================================================================================
# Constants based on the replication guide and provided data
# Emotion mapping
category_mapping = {'SD': 0, 'FD': 1, 'DD': 2, 'HD': 3, 'SAD': 4, 'AD': 5, 'ND': 6}

eeg_features = [f'neband_delta-sensor-{i}' for i in range(1, 33)] + \
               [f'neband_theta-sensor-{i}' for i in range(1, 33)] + \
               [f'neband_alpha-sensor-{i}' for i in range(1, 33)] + \
               [f'neband_beta-sensor-{i}' for i in range(1, 33)] + \
               [f'neband_gamma-sensor-{i}' for i in range(1, 33)]

# Other physiological signals
other_phys_features = [f'ne_ch{i}' for i in range(1, 33)]

# Time-related features
time_features = ['rec_time', 'utc']

features_columns = eeg_features + other_phys_features

print("Loading and processing data...")
if not os.path.exists(PHYSIO_FILE):
    raise FileNotFoundError(f"Data file not found: {PHYSIO_FILE}")
df = pd.read_csv(PHYSIO_FILE)

# Filter for relevant categories
original_rows = len(df)
df = df[df['category'].isin(category_mapping.keys())].copy()
print(f"Filtered for relevant categories: {len(df)} rows remaining from {original_rows}")

# Map categories to labels
df['label'] = df['category'].map(category_mapping)

# Drop unnecessary columns
df = df.drop(columns=['dataset_name', 'participant', 'category', 'Unnamed: 0'], errors='ignore')

# Convert time columns to numeric timestamps
df['rec_time'] = pd.to_datetime(df['rec_time'], errors='coerce').map(pd.Timestamp.timestamp)
df['utc'] = pd.to_numeric(df['utc'], errors='coerce')

# Handle missing values
df.dropna(inplace=True)
print(f"After dropping NaNs: {len(df)} rows remaining")

if len(df) == 0:
    raise ValueError("No data remaining after initial processing.")

# Select features and labels
X = df.drop('label', axis=1).values
y = df['label'].values

# Train/Val/Test Split (70/15/15)
print("Splitting data into train, validation, and test sets...")
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=SEED, stratify=y)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=SEED, stratify=y_temp)

# Feature Scaling (Fit on Train ONLY)
print("Standardizing features...")
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)

# Class Imbalance Handling (SMOTE on Train ONLY)
print("Applying SMOTE to the training set...")
smote_k_neighbors = min(5, pd.Series(y_train).value_counts().min() - 1)
if smote_k_neighbors > 0:
    smote = SMOTE(random_state=SEED, k_neighbors=smote_k_neighbors)
    X_train, y_train = smote.fit_resample(X_train, y_train)
    print("Class distribution after SMOTE:")
    print(pd.Series(y_train).value_counts())
else:
    print("Skipping SMOTE due to insufficient samples in a minority class.")

# Polynomial Features (as per replication guide) - Made optional
use_poly = False # Set to False to prevent memory issues
if use_poly:
    print("Generating quadratic polynomial features...")
    poly = PolynomialFeatures(degree=2, include_bias=False)
    X_train = poly.fit_transform(X_train)
    X_val = poly.transform(X_val)
    X_test = poly.transform(X_test)
    print(f"Feature dimension after polynomial expansion: {X_train.shape[1]}")
else:
    print("Skipping polynomial features.")

# Dataset and DataLoader
class EmotionDataset(Dataset):
    def __init__(self, features, labels):
        self.features = torch.tensor(features, dtype=torch.float32)
        self.labels = torch.tensor(labels, dtype=torch.long)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

batch_size = 128
train_dataset = EmotionDataset(X_train, y_train)
val_dataset = EmotionDataset(X_val, y_val)
test_dataset = EmotionDataset(X_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)

print(f"Train loader: {len(train_loader)} batches of size {batch_size}")
print(f"Validation loader: {len(val_loader)} batches")
print(f"Test loader: {len(test_loader)} batches")

# ==============================================================================================
# 2. Model Definition: Phy-FusionNet Components
# ==============================================================================================
class FourierPositionalEncoding(nn.Module):
    def __init__(self, embed_dim, max_len=512):
        super().__init__()
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim))
        pe = torch.zeros(1, max_len, embed_dim)
        pe[0, :, 0::2] = torch.sin(position * div_term)
        pe[0, :, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1)]
        return x

class MixtureOfHeadAttention(nn.Module):
    def __init__(self, embed_dim, num_heads, dropout=0.1):
        super().__init__()
        assert embed_dim % num_heads == 0
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.scaling = self.head_dim ** -0.5
        self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        self.dropout = nn.Dropout(dropout)
        self.head_gate = nn.Sequential(
            nn.Linear(embed_dim, num_heads),
            nn.Softmax(dim=-1)
        )

    def forward(self, x, freq_features=None):
        B, T, D = x.shape
        qkv = self.qkv_proj(x).chunk(3, dim=-1)
        q, k, v = map(lambda t: t.reshape(B, T, self.num_heads, self.head_dim).transpose(1, 2), qkv)

        q = q * self.scaling
        scores = torch.matmul(q, k.transpose(-2, -1))
        attn = torch.softmax(scores, dim=-1)
        attn = self.dropout(attn)

        head_out = torch.matmul(attn, v)
        head_out = head_out.transpose(1, 2)
        alpha = self.head_gate(x)
        weighted_out = alpha.unsqueeze(-1) * head_out
        out = weighted_out.reshape(B, T, D)
        return self.out_proj(out)

class ChannelAttention(nn.Module):
    def __init__(self, in_planes, ratio=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool1d(1)
        self.max_pool = nn.AdaptiveMaxPool1d(1)
        self.fc = nn.Sequential(
            nn.Conv1d(in_planes, in_planes // ratio, 1, bias=False),
            nn.ReLU(),
            nn.Conv1d(in_planes // ratio, in_planes, 1, bias=False)
        )
        self.sigmoid = nn.Sigmoid()
    def forward(self, x):
        avg_out = self.fc(self.avg_pool(x))
        max_out = self.fc(self.max_pool(x))
        out = avg_out + max_out
        return self.sigmoid(out)

class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        padding = kernel_size // 2
        self.conv1 = nn.Conv1d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()
    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x_cat = torch.cat([avg_out, max_out], dim=1)
        x_att = self.conv1(x_cat)
        return self.sigmoid(x_att)

class CBAM(nn.Module):
    def __init__(self, channels, reduction=16, kernel_size=7):
        super(CBAM, self).__init__()
        self.ca = ChannelAttention(channels, reduction)
        self.sa = SpatialAttention(kernel_size)
    def forward(self, x):
        x = x * self.ca(x)
        x = x * self.sa(x)
        return x

class MemoryEncoder(nn.Module):
    def __init__(self, embed_dim, memory_dim):
        super().__init__()
        self.conv = nn.Conv1d(embed_dim, memory_dim, kernel_size=3, padding=1)
        self.activation = nn.ReLU()
    def forward(self, x):
        return self.activation(self.conv(x))

class MemoryBank(nn.Module):
    def __init__(self, capacity, memory_dim):
        super().__init__()
        self.capacity = capacity
        self.memory_dim = memory_dim
        self.register_buffer('memory_queue', torch.zeros(capacity, memory_dim))
        self.register_buffer('ptr', torch.zeros(1, dtype=torch.long))
        self.is_full = False

    def add_memory(self, memory_features):
        batch_size = memory_features.size(0)
        ptr = int(self.ptr[0])
        if ptr + batch_size >= self.capacity:
            self.memory_queue[ptr:] = memory_features[:self.capacity - ptr]
            self.ptr[0] = 0
            self.is_full = True
        else:
            self.memory_queue[ptr:ptr + batch_size] = memory_features
            self.ptr[0] += batch_size

    def get_memory(self):
        if not self.is_full and self.ptr[0] == 0:
            return None
        return self.memory_queue[:int(self.ptr[0])] if not self.is_full else self.memory_queue

class MemoryConditionedAttention(nn.Module):
    def __init__(self, embed_dim, memory_dim, num_heads):
        super().__init__()
        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)
        self.mem_proj = nn.Linear(memory_dim, embed_dim)
    def forward(self, x, memory):
        if memory is None:
            return x
        memory = self.mem_proj(memory).unsqueeze(0).expand(x.size(0), -1, -1)
        out, _ = self.cross_attn(query=x, key=memory, value=memory)
        return out

class TransformerBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, dropout):
        super().__init__()
        self.attn = MixtureOfHeadAttention(embed_dim, num_heads, dropout)
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, embed_dim * 4), nn.GELU(),
            nn.Dropout(dropout), nn.Linear(embed_dim * 4, embed_dim)
        )
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, freq_features=None):
        x = x + self.dropout(self.attn(self.norm1(x), freq_features))
        x = x + self.dropout(self.ffn(self.norm2(x)))
        return x

class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt)**self.gamma * ce_loss
        return focal_loss.mean() if self.reduction == 'mean' else focal_loss.sum() if self.reduction == 'sum' else focal_loss

class PhyFusionNet(nn.Module):
    def __init__(self, input_dim, embed_dim, memory_dim, num_heads, num_layers, num_classes, dropout, memory_capacity):
        super().__init__()
        self.input_linear = nn.Linear(input_dim, embed_dim)
        self.pos_encoder = FourierPositionalEncoding(embed_dim)
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(embed_dim, num_heads, dropout) for _ in range(num_layers)
        ])
        self.cbam = CBAM(embed_dim)
        self.memory_encoder = MemoryEncoder(embed_dim, memory_dim)
        self.memory_bank = MemoryBank(memory_capacity, memory_dim)
        self.memory_conditioned_attn = MemoryConditionedAttention(embed_dim, memory_dim, num_heads)
        self.fc = nn.Linear(embed_dim, num_classes)
        self.gamma = nn.Parameter(torch.tensor(0.95))

    def forward(self, x):
        x = self.input_linear(x)
        x = x.unsqueeze(1)
        x = self.pos_encoder(x)
        
        stored_memory = self.memory_bank.get_memory()
        if stored_memory is not None:
            mem_conditioned_x = self.memory_conditioned_attn(x, stored_memory)
            x = x + mem_conditioned_x

        for block in self.transformer_blocks:
            x = block(x)
            
        seq_embedding = x.squeeze(1)
        x_reshaped_for_cbam = seq_embedding.unsqueeze(2)  # Shape: (B, embed_dim, 1)
        x_attended = self.cbam(x_reshaped_for_cbam)      # Shape: (B, embed_dim, 1)

        if self.training:
            encoded_mem = self.memory_encoder(x_attended).squeeze(2)
            self.memory_bank.add_memory(encoded_mem.detach())

        final_features = x_attended.squeeze(2) # Shape: (B, embed_dim)
        out = self.fc(final_features)
        return out

# ==============================================================================================
# 3. Training, Evaluation, and Optimization
# ==============================================================================================
# Set up device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")
EXP_LOG.put("hardware", "device", str(device))

# Define Loss Function
criterion = FocalLoss(alpha=1, gamma=2, reduction='mean').to(device)
EXP_LOG.put("config", "loss_function", {"type": "FocalLoss", "alpha": 1, "gamma": 2})

# Initialize GradScaler for mixed precision training (only when using CUDA)
scaler = GradScaler() if device.type == 'cuda' else None

# --- Training and Evaluation Functions ---
def train_epoch(model, dataloader, criterion, optimizer, device, scaler, max_grad_norm=1.0):
    """Trains the model for a single epoch."""
    model.train()
    running_loss = 0.0
    all_preds = []
    all_labels = []

    for X_batch, y_batch in dataloader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        optimizer.zero_grad()

        if scaler is not None:
            with autocast(device_type=device.type, dtype=torch.float16):
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
            scaler.step(optimizer)
            scaler.update()
        else:
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
            optimizer.step()

        running_loss += loss.item() * X_batch.size(0)
        _, predicted = torch.max(outputs.data, 1)
        all_preds.extend(predicted.cpu().numpy())
        all_labels.extend(y_batch.cpu().numpy())

    total_samples = len(all_labels)
    epoch_loss = running_loss / total_samples if total_samples > 0 else 0
    epoch_acc = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)
    return epoch_loss, epoch_acc, f1

def evaluate(model, dataloader, criterion, device):
    """Evaluates the model on a given dataset."""
    model.eval()
    running_loss = 0.0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for X_batch, y_batch in dataloader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            running_loss += loss.item() * X_batch.size(0)
            _, predicted = torch.max(outputs.data, 1)
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(y_batch.cpu().numpy())

    total_samples = len(all_labels)
    epoch_loss = running_loss / total_samples if total_samples > 0 else 0
    epoch_acc = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)
    return epoch_loss, epoch_acc, f1

# --------------------- Optuna Hyperparameter Optimization ---------------------
def objective(trial):
    """Objective function for Optuna to find the best hyperparameters."""
    # --- Suggest Hyperparameters ---
    embed_dim = trial.suggest_categorical('embed_dim', [128, 256])
    memory_dim = trial.suggest_categorical('memory_dim', [128, 256])
    num_heads_options = [h for h in [4, 8] if embed_dim % h == 0 and memory_dim % h == 0]
    if not num_heads_options: raise optuna.exceptions.TrialPruned("No valid head count for selected dims.")
    num_heads = trial.suggest_categorical('num_heads', num_heads_options)
    num_layers = trial.suggest_categorical('num_layers', [1, 2])
    dropout = trial.suggest_float('dropout', 0.2, 0.5)
    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)
    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-4, log=True)
    focal_gamma = trial.suggest_float('focal_gamma', 1.5, 3.0)

    # --- Model Initialization ---
    model_trial = PhyFusionNet(
        input_dim=X_train.shape[1], embed_dim=embed_dim, memory_dim=memory_dim,
        num_heads=num_heads, num_layers=num_layers, num_classes=len(category_mapping),
        dropout=dropout, memory_capacity=20
    ).to(device)

    criterion_trial = FocalLoss(alpha=1, gamma=focal_gamma).to(device)
    optimizer_trial = optim.AdamW(model_trial.parameters(), lr=learning_rate, weight_decay=weight_decay)
    scheduler_trial = optim.lr_scheduler.ReduceLROnPlateau(optimizer_trial, mode='max', factor=0.2, patience=5)
    scaler_trial = GradScaler() if device.type == 'cuda' else None

    # --- Training Loop ---
    num_epochs_trial = 30
    best_val_f1_trial = 0.0
    patience_trial = 7
    counter_trial = 0

    for epoch in range(num_epochs_trial):
        train_epoch(model_trial, train_loader, criterion_trial, optimizer_trial, device, scaler_trial)
        _, _, val_f1_trial = evaluate(model_trial, val_loader, criterion_trial, device)
        scheduler_trial.step(val_f1_trial)
        trial.report(val_f1_trial, epoch)
        if trial.should_prune():
            raise optuna.exceptions.TrialPruned()
        if val_f1_trial > best_val_f1_trial:
            best_val_f1_trial = val_f1_trial
            counter_trial = 0
        else:
            counter_trial += 1
            if counter_trial >= patience_trial:
                break
    return best_val_f1_trial

print("\n--- Starting Optuna Hyperparameter Optimization ---")
study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=SEED), pruner=optuna.pruners.MedianPruner())
study.optimize(objective, n_trials=10, timeout=3600) # Reduced trials for speed

try:
    best_trial = study.best_trial
except ValueError:
    best_trial = None

if best_trial:
    print("\nOptuna Optimization Finished.")
    print(f"Best Trial: {best_trial.number}, Best Validation F1-Score: {best_trial.value:.4f}")
    print("Best Hyperparameters:", best_trial.params)
    EXP_LOG.put("optuna", "best_params", best_trial.params)
    EXP_LOG.put("optuna", "best_val_f1", best_trial.value)
    try:
        vis.plot_optimization_history(study).write_image(os.path.join(OUTPUT_DIR, "optuna_history.png"))
        vis.plot_param_importances(study).write_image(os.path.join(OUTPUT_DIR, "optuna_importance.png"))
        EXP_LOG.add_artifact("optuna_plots", OUTPUT_DIR)
    except Exception as e:
        print(f"Could not generate Optuna visualization charts: {e}")
else:
    print("Optuna found no successful trials, will use default parameters.")
    best_params = {
        'embed_dim': 256, 'memory_dim': 256, 'num_heads': 8, 'num_layers': 2,
        'dropout': 0.4, 'learning_rate': 1e-4, 'weight_decay': 1e-5, 'focal_gamma': 2.0
    }
    EXP_LOG.put("optuna", "status", "Failed, using default params.")
    EXP_LOG.put("optuna", "best_params", best_params)

# --------------------- Final Training with Best Hyperparameters ---------------------
print("\n--- Starting final model training with best hyperparameters ---")
final_params = best_trial.params if best_trial else best_params
final_model = PhyFusionNet(
    input_dim=X_train.shape[1], num_classes=len(category_mapping),
    embed_dim=final_params['embed_dim'], memory_dim=final_params['memory_dim'],
    num_heads=final_params['num_heads'], num_layers=final_params['num_layers'],
    dropout=final_params['dropout'], memory_capacity=20
).to(device)

final_criterion = FocalLoss(alpha=1, gamma=final_params['focal_gamma']).to(device)
final_optimizer = optim.AdamW(final_model.parameters(), lr=final_params['learning_rate'], weight_decay=final_params['weight_decay'])
final_scheduler = optim.lr_scheduler.ReduceLROnPlateau(final_optimizer, mode='max', factor=0.2, patience=10, verbose=True)

num_epochs_final = 100
patience_final = 15
best_val_f1_final = 0.0
counter_final = 0
final_model_path = os.path.join(OUTPUT_DIR, 'final_best_model.pth')
writer_final = SummaryWriter(os.path.join(OUTPUT_DIR, 'runs/final_training'))

for epoch in range(num_epochs_final):
    train_loss, train_acc, train_f1 = train_epoch(final_model, train_loader, final_criterion, final_optimizer, device, scaler)
    val_loss, val_acc, val_f1 = evaluate(final_model, val_loader, final_criterion, device)

    print(f'Epoch {epoch+1}/{num_epochs_final} | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} F1: {train_f1:.4f} | '
          f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} F1: {val_f1:.4f}')

    writer_final.add_scalar('Loss/Train', train_loss, epoch)
    writer_final.add_scalar('Accuracy/Train', train_acc, epoch)
    writer_final.add_scalar('F1_Macro/Train', train_f1, epoch)
    writer_final.add_scalar('Loss/Val', val_loss, epoch)
    writer_final.add_scalar('Accuracy/Val', val_acc, epoch)
    writer_final.add_scalar('F1_Macro/Val', val_f1, epoch)
    writer_final.add_scalar('Learning_Rate', final_optimizer.param_groups[0]['lr'], epoch)

    final_scheduler.step(val_f1)

    if val_f1 > best_val_f1_final:
        best_val_f1_final = val_f1
        torch.save(final_model.state_dict(), final_model_path)
        print(f"  New best validation F1: {best_val_f1_final:.4f}. Model saved.")
        counter_final = 0
    else:
        counter_final += 1
        if counter_final >= patience_final:
            print("  Early stopping triggered.")
            break

writer_final.close()
EXP_LOG.put("metrics", "final_best_val_f1", best_val_f1_final)
EXP_LOG.add_artifact("final_model", final_model_path)

# --------------------- Final Model Evaluation ---------------------
if os.path.exists(final_model_path) and test_loader:
    print("\n--- Evaluating final model on the test set ---")
    final_model.load_state_dict(torch.load(final_model_path, map_location=device))
    test_loss, test_acc, test_f1 = evaluate(final_model, test_loader, final_criterion, device)

    print(f'Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc*100:.2f}% | Test F1-Score (Macro): {test_f1:.4f}')
    EXP_LOG.put("metrics", "test_loss", test_loss)
    EXP_LOG.put("metrics", "test_accuracy", test_acc)
    EXP_LOG.put("metrics", "test_f1_macro", test_f1)

    # Classification Report and Confusion Matrix
    y_true, y_pred = [], []
    with torch.no_grad():
        for features, labels in test_loader:
            features, labels = features.to(device), labels.to(device)
            outputs = final_model(features)
            _, predicted = torch.max(outputs.data, 1)
            y_true.extend(labels.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())

    class_names = list(category_mapping.keys())
    print("\nClassification Report (Test Set):")
    report = classification_report(y_true, y_pred, target_names=class_names, zero_division=0, digits=4)
    print(report)
    EXP_LOG.put("metrics", "classification_report", classification_report(y_true, y_pred, target_names=class_names, zero_division=0, output_dict=True))

    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    cm_path = os.path.join(OUTPUT_DIR, 'confusion_matrix.png')
    plt.savefig(cm_path)
    plt.close()
    print(f"Confusion matrix saved to {cm_path}")
    EXP_LOG.add_artifact("confusion_matrix", cm_path)
else:
    print("Final model or test data not found, skipping final evaluation.")

print("\n--- Script execution finished ---")
print(f"All outputs, logs, and models are saved in the '{OUTPUT_DIR}' directory.")

# Finalize the experiment log
EXP_LOG.finalize()